# 《科技写作与报告》`2024-09-26` `B106`

## P1

- 写作与发表周期

  - 定方向、找文献、定计划、做假设、数据支撑
  - AI for Science？

- 科研范式

  - 传统的科研范式：观察现象、总结假设、实验验证
    - 模型与理论的正确性需要给出实验验证
    - 诺贝尔奖只颁发给**得到证实的**理论工作
  - AI for Science
    - 一种新的科研范式
    - 与模型驱动相反，AI for Science 是一种数据驱动的科研范式

- 项目开始

  - 有一个好的想法，对背景进行充分的调研
  - 要有一个具体的可执行的路径，要能快速进行实验验证
  - 尽可能不要重复造轮子
  - 自己编写的代码应该考虑可复用性

- 寻找文献

  - 到具体的期刊中搜，或者在谷歌学术中搜
  - 有些会议中会有自己的网站
  - 可以在 arXiv 上订阅一些指定的话题
  - 跟踪一些相关方向的实验室主页
  - 使用引文索引

- 文献阅读

  - 如何快速阅读大量文献，找到各个文献的亮点

  - 使用文献管理工具，使用 AI 工具辅助分析文献

- 如何做文献综述

  - 开题报告中会有相关的段落
  - 学位论文中有相关章节
    - 读过的文献一定要进行整理
    - 综述文章一般引用率会比较高

## P2

- 什么样的总数是一个好综述
  - 现有工作、比较不同数据集的质量、分类法
  - 未来的研究方向
- 三年制硕士
  - 一般春节前完成答辩，有些同学会选择主动延毕
  - 写好论文可能需要以两个月时间，对学位论文的要求越来越高
- 学硕
  - 有学术论文的发表要求（前几年的学硕没有这个要求）
  - 小论文成为能否申请答辩的门槛（期刊论文随机性比会议大）
  - 根据自己导师的研究方向提前了解相关文献
  - 了解常见期刊以及会议的投稿时间
- 实验
  - 从最初的 idea 到最终成果可能中途目标会发生改变
  - 可以重新讲故事，以发现新的优势
  - 迎合数据指标的 trick
    - 数据清洗往往能让模型效果更好
    - 只在某些数据自己上效果更好的模型也并非没有价值
  - 实验分析也是十分重要的
  - 不要只使用单一指标衡量实验效果
  - 可以使用公开的 benchmark toolbox，使得自己可以聚焦于自己的工作
  - 很多工具迭代更新的速度很快，在复现论文是要选择正确的版本和配置
    - github issue 看看别人踩过什么坑
- 非实验性科研
  - 可以使用证明 + toy data 的思路
  - 但有优雅理论支撑的做法可能在实践中效果并不理想
    - ~~深有感触，让我想起了 ifft 实现拟拉东变换~~
- 假设
  - 描述性假设
  - 构造性假设

## P3

- 证明我们的假设
  - 偏理论得分析现象产生的原因
  - 使用饰演证明这个假设
- 四种论据
  - 分析、证明、仿真、实验
  - 尽可能使用能证实自己假设的方法，不要过分强调自己方法的全方面优势
    - 没有免费的午餐定理
  - 科学和伪科学的区别在与证据，科学是可证伪的
    - 能解释同一现象的模型往往很多，但是其中并不是所有模型都合理
- 同一个实验在不同系统中可能会模拟出不同的效果
  - 软硬件环境
  - 包版本、精度误差
- 假设检验方法
  - 统计与概率的课程中有
  - $k\sigma$ 原则、pvalue、tvalue 等假设检验方法
- 模型参数与超参数
  - 模型参数：算法从数据中学习到的参数
  - 超参数：影响模型，但并不由算法学习到的参数，一般有使用者基于经验给出
- 发表论文时
  - 应该选择合适的 baseline 进行比较
  - baseline 应该是目前效果比较好、有参考价值的工作
  - 不要回避和现有方法的比较
- 机器学习算法的检验应该要进行交叉验证？
  - 并计算均值和方差
  - 随着数据量越来越大，交叉验证的成本越来越高
  - 因此现在的很多研究并不进行交叉验证
  - 在一定程度上数据量的增大也能缓解模型摆动
- 关于实验
  - 过早的优化是万恶之源
  - 不要试图从零开始构建一个完整的系统
  - 能自动化完成的工作就不要手动完成
  - 可以开源自己的代码以方便他人重用和复现

